{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yud-0KmgjVmg"
      },
      "source": [
        "# Hands-On LoRA\n",
        "Learn how to fine-tune LLM on a custom dataset using LoRA.\n",
        "\n",
        "Meta’s Llama 3.1 model is used for a variety of use cases, including question-answering, text generation, code generation, story writing, and much more. One use case also involves solving math word problems, but the model usually provides solutions in natural language rather than pure math expressions. We want to fine-tune the Llama 3.1 model to provide solutions to word problems in mathematical expressions.\n",
        "\n",
        ">We will use the openai/gsm8k dataset from Hugging Face for fine-tuning. GSM8K (Grade School Math 8K) is a dataset of 8.5K grade-school math word problems involving multi-step reasoning along with their solutions in pure maths expressions.\n",
        "\n",
        "Let’s begin with the journey of fine-tuning Meta’s Llama 3.1 model on openai/gsm8k dataset using LoRA.\n",
        "\n",
        "## Install the dependencies\n",
        "First, let’s install the libraries required for fine-tuning. We'll be installing the latest versions (at the time of writing) of the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH9pJE0ijNrq",
        "outputId": "32c10283-3e56-4117-9668-167c8cb608f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.44.1 in /usr/local/lib/python3.11/dist-packages (4.44.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (0.5.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.1) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.1) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.44.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.1) (2025.1.31)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.29.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Requirement already satisfied: bitsandbytes==0.43.3 in /usr/local/lib/python3.11/dist-packages (0.43.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.43.3) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.43.3) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->bitsandbytes==0.43.3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->bitsandbytes==0.43.3) (3.0.2)\n",
            "Requirement already satisfied: datasets==2.21.0 in /usr/local/lib/python3.11/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (0.29.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.21.0) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.21.0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.21.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.21.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.21.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.21.0) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.21.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.21.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.21.0) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.17.0)\n",
            "Requirement already satisfied: trl==0.9.6 in /usr/local/lib/python3.11/dist-packages (0.9.6)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (4.44.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (1.26.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (2.21.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.11/dist-packages (from trl==0.9.6) (0.9.16)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->trl==0.9.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.9.6) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.29.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.5.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->trl==0.9.6) (4.67.1)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (1.7.1)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro>=0.5.11->trl==0.9.6) (4.4.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->trl==0.9.6) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->trl==0.9.6) (3.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->trl==0.9.6) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->trl==0.9.6) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->trl==0.9.6) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->trl==0.9.6) (2025.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.9.6) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.9.6) (1.17.0)\n",
            "Requirement already satisfied: peft==0.12.0 in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (4.44.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (1.3.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (0.5.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.12.0) (0.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.12.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.12.0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.12.0) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.12.0) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.12.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2025.1.31)\n",
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (4.12.2)\n",
            "Requirement already satisfied: InquirerPy==0.3.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[cli]) (2025.1.31)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.44.1\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes==0.43.3\n",
        "!pip install datasets==2.21.0\n",
        "!pip install trl==0.9.6\n",
        "!pip install peft==0.12.0\n",
        "!pip install -U \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN2c1e9BjjJR"
      },
      "source": [
        "- Line 1: We install the transformers library, which is a Hugging Face library that provides APIs and tools to download and train state-of-the-art pretrained models.\n",
        "\n",
        "- Line 2: We install the accelerate library, which is designed to facilitate training deep learning models across different hardware. It enables the training and inference to be simple, efficient, and adaptable.\n",
        "\n",
        "- Line 3: We install the bitsandbytes library, which is a transformers library that helps with the quantization of the model.\n",
        "\n",
        "- Line 4: We install the dataset library for sharing and accessing datasets for downstream tasks.\n",
        "\n",
        "- Line 5: We install the trl library for training transformer models with reinforcement learning and supervised fine-tuning.\n",
        "\n",
        "- Line 6: We install the peft library for parametric efficient fine-tuning of large language models for downstream tasks.\n",
        "\n",
        "- Line 7: We install the Hugging Face CLI to log in and access the model and dataset from Hugging Face.\n",
        "\n",
        "## Hugging Face CLI\n",
        "After installing the required libraries, it's time to log in to the Hugging Face CLI. Hugging Face requires this step to access any model and dataset from the Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbPPlPtMjhMV",
        "outputId": "dad52194-3861-482d-eafb-d031b8d6f377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `vllm` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `vllm`\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "login(token=os.getenv(\"HF_TOKEN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT-RVtL7kEi-"
      },
      "source": [
        "## Quantization\n",
        "Now, let’s load the pretrained model with quantization and see how it responds to a math word problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2qFsqEULKrz",
        "outputId": "a92cfe48-db12-4d59-dc20-c9fc4c323110"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYKeaozOLgN8",
        "outputId": "a6e9c9ca-82ce-4c2d-cb51-d6260e6df3da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Archive:  /content/meta-llamaLlama-3.1-8B.zip\n",
            "   creating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/\n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/config.json  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/generation_config.json  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/gitattributes  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/LICENSE  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/model-00001-of-00004.safetensors  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/model-00002-of-00004.safetensors  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/model-00003-of-00004.safetensors  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/model-00004-of-00004.safetensors  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/model.safetensors.index.json  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/README.md  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/special_tokens_map.json  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/tokenizer.json  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/tokenizer_config.json  \n",
            "  inflating: /content/llama3-1-8B/meta-llamaLlama-3.1-8B/USE_POLICY.md  \n"
          ]
        }
      ],
      "source": [
        "# Copia il file ZIP dalla cartella di Google Drive alla memoria di Colab\n",
        "!cp /content/drive/MyDrive/meta-llamaLlama-3.1-8B.zip /content/\n",
        "\n",
        "# Vai nella cartella di destinazione (non necessario, ma utile)\n",
        "%cd /content/\n",
        "\n",
        "# Estrai il file ZIP\n",
        "!unzip /content/meta-llamaLlama-3.1-8B.zip -d /content/llama3-1-8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKQXyFp7PFdc",
        "outputId": "f9be1385-3b5f-4215-d2ca-e1141c3abafc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[34;42mmeta-llamaLlama-3.1-8B\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls /content/llama3-1-8B/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a21d6191e93d47d8b9f05e9136681728",
            "b2fbda09de6446c99ae80198f51ff6be",
            "bd464e72edf84a2bb14dca0081c764db",
            "c5faf72282f7493eb130652ed41386e6",
            "a7f4f953626a4618861c8ce446c2b29d",
            "8b19ac579f794ad6b95db63363c6f556",
            "fb39a17f614e4a1f82e7d39dd27c1b75",
            "601929dddf2b4d598beb44662948bb53",
            "52af64f6897a498897add16deb32e231",
            "51d0db8b89ac4a8e8f3a34c915226997",
            "5bb8bfe682b34358914c2fd44c14f555"
          ]
        },
        "id": "YRM_Y7OqkAxl",
        "outputId": "76bfa9ce-82ba-4fdf-837a-49e0dcf8e977"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a21d6191e93d47d8b9f05e9136681728",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "#bnb_config = BitsAndBytesConfig(\n",
        "#    load_in_8bit=True\n",
        "#)\n",
        "\n",
        "model_path = \"/content/llama3-1-8B/meta-llamaLlama-3.1-8B\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwOjhIktafcE",
        "outputId": "3af2f15f-ecd2-47de-dd21-3b2a396476f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter dtypes: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\n"
          ]
        }
      ],
      "source": [
        "param_dtypes = [param.dtype for param in model.parameters()]\n",
        "print(\"Parameter dtypes:\", param_dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ1EKg_vRqTT",
        "outputId": "090968fd-517e-4ff2-fa28-e2641b35531b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32121053440\n"
          ]
        }
      ],
      "source": [
        "print(model.get_memory_footprint())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVanrakzRcbk",
        "outputId": "f11c3f79-3971-4409-d1b6-3fa302da56bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogheter in April and May? A. 96 B. 48 C. 24 D. 12\\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogheter in April and May? A. 96 B. 48 C. 24 D. 12']\n"
          ]
        }
      ],
      "source": [
        "input = tokenizer(\"Natalia sold clips to 48 of her friends in April, and then she sold half as \\\n",
        "many clips in May. How many clips did Natalia sell altogheter in April and May?\", return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "response = model.generate(**input, max_new_tokens=100)\n",
        "print(tokenizer.batch_decode(response, skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9J5k3ZYfl-Y"
      },
      "source": [
        "- Line 1: We import the AutoModelForCausalLM, AutoTokenizer, and BitsAndBytesConfig modules from the transformers library.\n",
        "\n",
        "- Line 2: We load the PyTorch library for GPU acceleration.\n",
        "\n",
        "- Lines 4–6: We apply the 8-bit quantization to the model using the BitesAndBytesConfig class from the transformers library.\n",
        "\n",
        "- Lines 8–11: We load the model using the from_pretrained() function of the AutoModelForCausalLM class.\n",
        "\n",
        "- Lines 13–14: We load the tokenizer for our model and tokenize the input prompt for inference of the pretrained model.\n",
        "\n",
        "- Lines 16–17: We pass the tokenized input to the model and print the generated response by converting it to text while ignoring special tokens.\n",
        "\n",
        "We can see that the model provided a solution to the math problem in natural language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te2ZMe8pgccO"
      },
      "source": [
        "# Training the model\n",
        "Now, let’s use the openai/gsm8k dataset to fine-tune the model so it generates responses in mathematical expressions.\n",
        "\n",
        "# Preprocess the dataset\n",
        "Let’s begin by preprocessing the dataset for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "df2edc009b664660a08e0f5d8a50ca19",
            "fa1861ff01524bb1a402cee40e01a7af",
            "87c815ba99b74f5dbb673ceae2a599be",
            "54a5113c4d6b400f8a0f3f2f47ba95c3",
            "5242f778c16a4507928dda308c6373f7",
            "6ff5d29a433345deac2847a3b8617a56",
            "b5eeb94bb76141f6ba9e0665f43f1e8d",
            "3308e4b6423945c6bf684f89dc72a24d",
            "d7cc77a3936d4c8bb58e50130ef4e85e",
            "03474e6c30634d3f99a1a9e8920a08b0",
            "b4dd0adac58d4b058684e1751d89af0d"
          ]
        },
        "id": "mWwekvMsi44T",
        "outputId": "4ee4189d-8ccb-4b51-b1bb-d2494933f28c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df2edc009b664660a08e0f5d8a50ca19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 400\n",
              "})"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = \"openai/gsm8k\"\n",
        "\n",
        "data = load_dataset(dataset, \"main\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "data = data.map(lambda samples: tokenizer(samples['question'], samples['answer'], truncation=True, padding=\"max_length\", max_length=100), batched=True)\n",
        "\n",
        "train_sample = data[\"train\"].select(range(400))\n",
        "\n",
        "display(train_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoDmF9fMt4dv"
      },
      "source": [
        "- Line 1: We import the load_dataset from the datasets library.\n",
        "\n",
        "- Lines 3–4: We load the openai/gsm8k dataset from the Hugging Face using the load_dataset function.\n",
        "\n",
        "- Line 6: We specified the pad token to be the eos_token of the tokenizer. The eos_token is a special token that represents the end of a sentence.\n",
        "\n",
        "- Line 7: We tokenize columns from the dataset, which we want to use for fine-tuning. Our dataset consists of only two columns, \"question\" and \"answer\", and we are using both of them to create samples. We set the truncation=True and padding=\"max_length\" where max_length=100 for a fixed-length dataset.\n",
        "\n",
        "- Line 8: We create the training dataset train_sample by selecting only 400 rows from the complete dataset for fine-tuning. We used very few rows for training the model to reduce the training time.\n",
        "\n",
        "- Line 10: We display the information of the training dataset train_sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LTfRLuHxMkh"
      },
      "source": [
        "We can see that our dataset has 400 rows and contains input_ids and attention_mask columns along with question and answer columns for each row.\n",
        "\n",
        "Let’s print a row of the training dataset train_sample to see the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lthlPpPWuBM0",
        "outputId": "6d264c4c-20b7-468c-e67e-8637acd33004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'question': ['Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'], 'answer': ['Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'], 'input_ids': [[128000, 45, 4306, 689, 6216, 27203, 311, 220, 2166, 315, 1077, 4885, 304, 5936, 11, 323, 1243, 1364, 6216, 4376, 439, 1690, 27203, 304, 3297, 13, 2650, 1690, 27203, 1550, 42701, 689, 4662, 31155, 304, 5936, 323, 3297, 30, 128000, 45, 4306, 689, 6216, 220, 2166, 14, 17, 284, 1134, 2166, 14, 17, 28, 1187, 2511, 1187, 27203, 304, 3297, 627, 45, 4306, 689, 6216, 220, 2166, 10, 1187, 284, 1134, 2166, 10, 1187, 28, 5332, 2511, 5332, 27203, 31155, 304, 5936, 323, 3297, 627, 827, 220, 5332, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "print(train_sample[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XcKfjAjy2AC"
      },
      "source": [
        "We can see the values of input_ids and attention_mask against the first row of the dataset. Our dataset is now ready for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va3as3Vey49m"
      },
      "source": [
        "# LoRA configurations\n",
        "Now, let’s set the LoRA configurations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4r8xZixDx_zW"
      },
      "outputs": [],
      "source": [
        "import peft\n",
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxTp_Wne2lju"
      },
      "source": [
        "### 📌 **Spiegazione della Configurazione LoRA (Low-Rank Adaptation)**\n",
        "La configurazione che hai fornito è usata per applicare **LoRA (Low-Rank Adaptation)** a un modello di **language modeling causale (CAUSAL_LM)**, come LLaMA o GPT, per **fine-tuning efficiente**.\n",
        "\n",
        "LoRA permette di **adattare un modello di grandi dimensioni** senza dover aggiornare tutti i parametri, riducendo drasticamente il **costo computazionale** e la memoria necessaria.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **Analisi dei parametri**\n",
        "```python\n",
        "import peft\n",
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "```\n",
        "\n",
        "### 🔹 **1️⃣ `r=16` → Rank della decomposizione LoRA**\n",
        "- `r` indica il **rank** della **fattorizzazione a bassa dimensione** che LoRA utilizza per ridurre il numero di parametri da aggiornare.\n",
        "- Un valore **più alto** di `r` aumenta la capacità di apprendimento del modello, ma consuma più memoria.\n",
        "- **Valori tipici:** `r=8` o `r=16` per un buon equilibrio tra efficienza e accuratezza.\n",
        "\n",
        "📌 **Cosa significa?**  \n",
        "LoRA sostituisce i pesi di una matrice \\( W \\) con due matrici più piccole \\( A \\) e \\( B \\), ciascuna di dimensione **ridotta** (`r`).  \n",
        "Questo riduce drasticamente il numero di parametri da aggiornare durante il training.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **2️⃣ `lora_alpha=16` → Fattore di scalatura**\n",
        "- `lora_alpha` controlla l'**importanza** degli aggiornamenti LoRA rispetto ai pesi originali.\n",
        "- Più alto è `lora_alpha`, più forte è l'impatto delle modifiche LoRA.\n",
        "\n",
        "📌 **Come funziona?**  \n",
        "Il valore **effettivo** degli aggiornamenti viene scalato come:\n",
        "\\[\n",
        "\\text{Update} = \\frac{\\text{LoRA Modifications} \\times \\text{lora_alpha}}{r}\n",
        "\\]\n",
        "Per questa configurazione:\n",
        "\\[\n",
        "\\frac{16}{16} = 1\n",
        "\\]\n",
        "Quindi le modifiche vengono **applicate senza riduzione**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **3️⃣ `target_modules=[\"q_proj\", \"v_proj\"]` → Moduli su cui applicare LoRA**\n",
        "- Indica **quali layer del modello** saranno ottimizzati con LoRA.\n",
        "- `\"q_proj\"` e `\"v_proj\"` sono **i proiettori delle matrici di attenzione** nei Transformer (Query e Value).\n",
        "\n",
        "📌 **Perché solo questi moduli?**  \n",
        "Questa scelta **riduce i parametri da aggiornare**, migliorando efficienza e prestazioni.  \n",
        "I Transformer utilizzano **Query-Key-Value Attention**, quindi modificare solo `q_proj` e `v_proj` permette di migliorare la rappresentazione **senza modificare completamente l'architettura**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **4️⃣ `lora_dropout=0.1` → Dropout per regolarizzazione**\n",
        "- `lora_dropout=0.1` applica **dropout** del **10%** sui pesi LoRA.\n",
        "- Aiuta ad **evitare overfitting** durante il fine-tuning.\n",
        "\n",
        "📌 **Quando usare il dropout?**\n",
        "- Per dataset **piccoli**, è utile per **migliorare la generalizzazione**.\n",
        "- Per dataset **grandi**, può essere impostato a `0` per massimizzare l'apprendimento.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **5️⃣ `bias=\"none\"` → Nessun aggiornamento del bias**\n",
        "- `\"none\"` significa che **LoRA non aggiornerà i bias** nei layer del modello.\n",
        "- Puoi usare:\n",
        "  - `\"none\"` → Bias **non modificati** (migliore per risparmiare memoria)\n",
        "  - `\"all\"` → Aggiorna tutti i bias\n",
        "  - `\"lora_only\"` → Aggiorna solo i bias nei moduli modificati con LoRA\n",
        "\n",
        "📌 **Perché non aggiornare i bias?**  \n",
        "I bias occupano poca memoria e **non hanno un grande impatto** sulle prestazioni.  \n",
        "Quindi, per **efficienza**, spesso **non vengono toccati** nel fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **6️⃣ `task_type=\"CAUSAL_LM\"` → Modello autoregressivo (GPT, LLaMA)**\n",
        "- Indica che il modello è un **language model causale** (come **LLaMA, GPT, Mistral**).\n",
        "- Se fosse un modello diverso, potresti usare:\n",
        "  - `\"SEQ2SEQ_LM\"` → Modelli encoder-decoder (T5, BART)\n",
        "  - `\"TOKEN_CLASSIFICATION\"` → Named Entity Recognition (NER)\n",
        "  - `\"TEXT_CLASSIFICATION\"` → Classificazione di testi\n",
        "\n",
        "📌 **Perché \"CAUSAL_LM\"?**  \n",
        "Nei **language model causali**, ogni token può **vedere solo i token precedenti** durante la generazione.  \n",
        "Esempi di modelli con **Causal LM**:\n",
        "- **GPT-3, GPT-4**\n",
        "- **LLaMA 3.1**\n",
        "- **Mistral**\n",
        "- **Falcon**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🚀 Riepilogo: Cosa fa questa configurazione?**\n",
        "| **Parametro**    | **Significato** |\n",
        "|-----------------|----------------|\n",
        "| `r=16`          | Riduzione della dimensione delle matrici LoRA (rank) |\n",
        "| `lora_alpha=16` | Controlla la forza delle modifiche LoRA |\n",
        "| `target_modules=[\"q_proj\", \"v_proj\"]` | Applica LoRA solo ai moduli di attenzione (Query, Value) |\n",
        "| `lora_dropout=0.1` | Evita overfitting riducendo del 10% l'attivazione di LoRA |\n",
        "| `bias=\"none\"` | Non aggiorna i bias per risparmiare memoria |\n",
        "| `task_type=\"CAUSAL_LM\"` | Il modello è un **language model autoregressivo** |\n",
        "\n",
        "✅ **Questa configurazione rende il fine-tuning di un modello come LLaMA 3.1 8B molto più efficiente!** 🚀  \n",
        "Dimmi se vuoi approfondire qualcosa! 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apu0FbOW22vS"
      },
      "source": [
        "# Set the training arguments\n",
        "After setting the LoRA configurations, let’s set the training arguments for training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fu8IwyST2oIK"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "working_dir = \"/content/llama3-1-8B/\"\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"lora\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_directory,\n",
        "    auto_find_batch_size = True,\n",
        "    learning_rate = 3e-4,\n",
        "    num_train_epochs = 5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL9RiWE15OcX"
      },
      "source": [
        "### 📌 **Spiegazione della Configurazione `TrainingArguments`**\n",
        "Il codice imposta gli **argomenti di training** per addestrare un modello con **Hugging Face Transformers**.  \n",
        "In particolare, **configura il fine-tuning di LLaMA 3.1 con LoRA**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **Analisi dei Parametri**\n",
        "```python\n",
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "working_dir = \"/content/llama3-1-8B/\"\n",
        "\n",
        "output_directory = os.path.join(working_dir, \"lora\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_directory,\n",
        "    auto_find_batch_size = True,\n",
        "    learning_rate = 3e-4,\n",
        "    num_train_epochs = 5\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **1️⃣ `output_dir = output_directory` → Cartella per salvare il modello**\n",
        "```python\n",
        "output_directory = os.path.join(working_dir, \"lora\")\n",
        "output_dir = output_directory\n",
        "```\n",
        "📌 **Cosa fa?**  \n",
        "- Definisce la **cartella in cui salvare i checkpoint** del modello durante e dopo il training.\n",
        "- In questo caso, i pesi addestrati con LoRA verranno salvati in:\n",
        "  ```\n",
        "  /content/llama3-1-8B/lora/\n",
        "  ```\n",
        "- Dopo il training, potrai **riutilizzare i pesi LoRA da questa cartella**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **2️⃣ `auto_find_batch_size = True` → Adatta automaticamente il batch size**\n",
        "```python\n",
        "auto_find_batch_size = True\n",
        "```\n",
        "📌 **Cosa fa?**  \n",
        "- **Trova automaticamente il batch size massimo** che la GPU può gestire **senza superare la memoria disponibile**.\n",
        "- Questo è utile perché:\n",
        "  - I modelli **LLaMA 3.1 e GPT sono enormi** e possono facilmente esaurire la VRAM.\n",
        "  - Se il batch size è **troppo alto**, il training si interrompe per **Out of Memory (OOM)**.\n",
        "  - Con **`auto_find_batch_size=True`**, il batch viene regolato automaticamente in base alla tua GPU.\n",
        "\n",
        "✅ **Evita crash per OOM su GPU con memoria limitata!** 🚀\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **3️⃣ `learning_rate = 3e-4` → Tasso di apprendimento**\n",
        "```python\n",
        "learning_rate = 3e-4\n",
        "```\n",
        "📌 **Cosa fa?**  \n",
        "- **Determina quanto velocemente il modello aggiorna i pesi durante l'addestramento.**\n",
        "- `3e-4` significa **0.0003**, che è un valore piuttosto alto per LoRA.\n",
        "- Solitamente:\n",
        "  - **`1e-5` o `2e-5`** per fine-tuning **completo** di modelli grandi.\n",
        "  - **`3e-4`** è più alto perché **LoRA aggiorna meno parametri**, quindi può apprendere più velocemente.\n",
        "\n",
        "✅ **Tasso di apprendimento ottimizzato per LoRA!** 🚀\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 **4️⃣ `num_train_epochs = 5` → Numero di epoche di training**\n",
        "```python\n",
        "num_train_epochs = 5\n",
        "```\n",
        "📌 **Cosa fa?**  \n",
        "- **Indica quante volte il modello deve passare sui dati di training.**\n",
        "- **Più epoche = migliore apprendimento**, ma **può causare overfitting**.\n",
        "- Solitamente:\n",
        "  - **`3-5` epoche per fine-tuning standard.**\n",
        "  - **`10+` epoche per dataset piccoli o modelli più piccoli.**\n",
        "  \n",
        "✅ **5 epoche sono un buon compromesso per il fine-tuning con LoRA!**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **🚀 Riepilogo: Cosa fa questa configurazione?**\n",
        "| **Parametro**            | **Significato** |\n",
        "|------------------------|----------------|\n",
        "| `output_dir=\"lora/\"`  | Salva i pesi LoRA in `/content/llama3-1-8B/lora/` |\n",
        "| `auto_find_batch_size=True` | Regola automaticamente il batch size per evitare errori di memoria |\n",
        "| `learning_rate=3e-4`  | Tasso di apprendimento più alto per LoRA (più veloce rispetto al fine-tuning completo) |\n",
        "| `num_train_epochs=5`  | 5 epoche per bilanciare apprendimento e overfitting |\n",
        "\n",
        "✅ **Questa configurazione permette di eseguire il fine-tuning di LLaMA 3.1 8B con LoRA in modo efficiente!** 🚀  \n",
        "Dimmi se vuoi aggiungere qualche dettaglio! 😊"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKAmHsDN5aPp"
      },
      "source": [
        "# Set the trainer\n",
        "Let’s set the trainer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRIBN4EE5PKm",
        "outputId": "017c8a0a-8431-4801-c0bd-4239d6f14aba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_sample,\n",
        "    peft_config = lora_config,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0QN_dhN8IO-"
      },
      "source": [
        "- Line 1: We import the transformers library.\n",
        "\n",
        "- Line 2: We import the SFTTrainer class from the trl library.\n",
        "\n",
        "- Lines 4–11: We set the configurations for the trainer using the SFTTrainer class.\n",
        "\n",
        "- Line 5: We specify the training model using the model parameter.\n",
        "\n",
        "- Line 6: We specify the training arguments using the args parameter.\n",
        "\n",
        "- Line 7: We specify the training data using the train_dataset parameter.\n",
        "\n",
        "- Line 8: We specify the LoRA configurations using the peft_config parameter.\n",
        "\n",
        "- Line 9: We specify the tokenizer for the model.\n",
        "\n",
        "- Line 10: We specify the data_collator for creating batches from the set of training data.\n",
        "\n",
        "Now, we are all set for training the models. We can train the model using the trainer.train() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "qxBSBJIn8P3-",
        "outputId": "86fea96f-d6c4-4252-afc2-a1bb563e5bf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelipaosfdk\u001b[0m (\u001b[33mfelipaosfdk-university-of-udine\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250223_132000-au3ljb7p</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/felipaosfdk-university-of-udine/huggingface/runs/au3ljb7p' target=\"_blank\">/content/llama3-1-8B/lora</a></strong> to <a href='https://wandb.ai/felipaosfdk-university-of-udine/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/felipaosfdk-university-of-udine/huggingface' target=\"_blank\">https://wandb.ai/felipaosfdk-university-of-udine/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/felipaosfdk-university-of-udine/huggingface/runs/au3ljb7p' target=\"_blank\">https://wandb.ai/felipaosfdk-university-of-udine/huggingface/runs/au3ljb7p</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  2/250 : < :, Epoch 0.02/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:34, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.200600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=1.2006151123046875, metrics={'train_runtime': 395.4619, 'train_samples_per_second': 5.057, 'train_steps_per_second': 1.264, 'total_flos': 9050144853196800.0, 'train_loss': 1.2006151123046875, 'epoch': 5.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jviRkX43-ip-"
      },
      "source": [
        "After training, we can save the fine-tuned model on our local machines for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1jt8CswX8j1l"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "model_path = os.path.join(output_directory, f\"lora_model\")\n",
        "trainer.model.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3zr1zu1_pRe",
        "outputId": "741b0311-7bb8-41fd-ff64-e78e8ff3dca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/llama3-1-8B/lora/ (stored 0%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/ (stored 0%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/scheduler.pt (deflated 56%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/tokenizer_config.json (deflated 96%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/special_tokens_map.json (deflated 64%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/tokenizer.json (deflated 74%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/trainer_state.json (deflated 56%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/training_args.bin (deflated 51%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/README.md (deflated 66%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/adapter_config.json (deflated 51%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/optimizer.pt (deflated 9%)\n",
            "  adding: content/llama3-1-8B/lora/checkpoint-500/rng_state.pth (deflated 25%)\n",
            "  adding: content/llama3-1-8B/lora/lora_model/ (stored 0%)\n",
            "  adding: content/llama3-1-8B/lora/lora_model/adapter_model.safetensors (deflated 7%)\n",
            "  adding: content/llama3-1-8B/lora/lora_model/README.md (deflated 66%)\n",
            "  adding: content/llama3-1-8B/lora/lora_model/adapter_config.json (deflated 51%)\n",
            "  adding: content/llama3-1-8B/lora/runs/ (stored 0%)\n",
            "  adding: content/llama3-1-8B/lora/runs/Feb23_13-00-29_a425d3e5fadb/ (stored 0%)\n",
            "  adding: content/llama3-1-8B/lora/runs/Feb23_13-00-29_a425d3e5fadb/events.out.tfevents.1740316492.a425d3e5fadb.6324.0 (deflated 78%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/llama3-1-8B/lora.zip /content/llama3-1-8B/lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5OV274C9Ao6M",
        "outputId": "6a91a8d9-3347-4eb1-c4a9-8479c6db42be"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_ed4f8991-4533-444b-b953-70c583145d89\", \"lora.zip\", 102823192)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/llama3-1-8B/lora.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CdJRrKHB7fg"
      },
      "source": [
        "# Load the fine-tuned model#\n",
        "Let’s load the already trained model saved on our machine to see the inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "e84b08d16ad141bea14243ba4154b5c3",
            "2d8b459e8e0e4624b006151709b4993c",
            "3d11ad7d361e43b285ab4637018e5d00",
            "145a67d712784079b572ed3108fdce4d",
            "8cdd83193ca243c296cede606c9c3e87",
            "73cffe4a5aaf4a51b272edf11eb08419",
            "7d0d3e2bface445f9ba909f8b8a43eee",
            "9a87c44a11764894ba8e1cc6c1452c26",
            "e6b9dda1de284a97ab1e95005cf93921",
            "80bc43175f3a4483a983ccd61c2a0575",
            "9877212194ff46c3b3c90ed9fe496517"
          ]
        },
        "id": "i0aHc2FnA1F4",
        "outputId": "36055d7e-5e56-4952-96f0-81e93d450c36"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e84b08d16ad141bea14243ba4154b5c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?The number of clips Natalia sold in May is 48/2 = <<48/2=24>>24 clips.\n",
            "So, together in April and May, Natalia sold 48+24 = <<48+24=72>>72 clips.\n",
            "#### 72 clips\n",
            "72 clips.\n",
            "#### 72 clips\n",
            "72 clips.\n",
            "#### 72 clips\n",
            "72 clips.\n",
            "#### 72 clips\n",
            "72 clips.\n",
            "#### 72 clips\n",
            "72 clips.\n",
            "#### 72 clips\n",
            "72 clips.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Percorso del modello base\n",
        "base_model_path = \"/content/llama3-1-8B/meta-llamaLlama-3.1-8B\"\n",
        "# Percorso del modello LoRa addestrato\n",
        "lora_model_path = \"/content/llama3-1-8B/lora/lora_model\"\n",
        "\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    lora_model_path,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Carica il tokenizer dal modello base\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Tokenizza il testo e sposta gli input sulla GPU\n",
        "input_text = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
        "input_tokens = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Sposta su GPU\n",
        "\n",
        "response = loaded_model.generate(**input_tokens, max_new_tokens=100)\n",
        "print(tokenizer.decode(response[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzLBaiq4GqdV",
        "outputId": "ed2e173a-4f45-48eb-a60b-8d533ad71f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Feb 23 14:14:59 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             49W /  400W |   31109MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxtmy4EVG4U1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03474e6c30634d3f99a1a9e8920a08b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "145a67d712784079b572ed3108fdce4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80bc43175f3a4483a983ccd61c2a0575",
            "placeholder": "​",
            "style": "IPY_MODEL_9877212194ff46c3b3c90ed9fe496517",
            "value": " 4/4 [00:14&lt;00:00,  3.08s/it]"
          }
        },
        "2d8b459e8e0e4624b006151709b4993c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73cffe4a5aaf4a51b272edf11eb08419",
            "placeholder": "​",
            "style": "IPY_MODEL_7d0d3e2bface445f9ba909f8b8a43eee",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3308e4b6423945c6bf684f89dc72a24d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d11ad7d361e43b285ab4637018e5d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a87c44a11764894ba8e1cc6c1452c26",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6b9dda1de284a97ab1e95005cf93921",
            "value": 4
          }
        },
        "51d0db8b89ac4a8e8f3a34c915226997": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5242f778c16a4507928dda308c6373f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52af64f6897a498897add16deb32e231": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54a5113c4d6b400f8a0f3f2f47ba95c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03474e6c30634d3f99a1a9e8920a08b0",
            "placeholder": "​",
            "style": "IPY_MODEL_b4dd0adac58d4b058684e1751d89af0d",
            "value": " 7473/7473 [00:01&lt;00:00, 6955.31 examples/s]"
          }
        },
        "5bb8bfe682b34358914c2fd44c14f555": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "601929dddf2b4d598beb44662948bb53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ff5d29a433345deac2847a3b8617a56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73cffe4a5aaf4a51b272edf11eb08419": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d0d3e2bface445f9ba909f8b8a43eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80bc43175f3a4483a983ccd61c2a0575": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87c815ba99b74f5dbb673ceae2a599be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3308e4b6423945c6bf684f89dc72a24d",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7cc77a3936d4c8bb58e50130ef4e85e",
            "value": 7473
          }
        },
        "8b19ac579f794ad6b95db63363c6f556": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cdd83193ca243c296cede606c9c3e87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9877212194ff46c3b3c90ed9fe496517": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a87c44a11764894ba8e1cc6c1452c26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a21d6191e93d47d8b9f05e9136681728": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2fbda09de6446c99ae80198f51ff6be",
              "IPY_MODEL_bd464e72edf84a2bb14dca0081c764db",
              "IPY_MODEL_c5faf72282f7493eb130652ed41386e6"
            ],
            "layout": "IPY_MODEL_a7f4f953626a4618861c8ce446c2b29d"
          }
        },
        "a7f4f953626a4618861c8ce446c2b29d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2fbda09de6446c99ae80198f51ff6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b19ac579f794ad6b95db63363c6f556",
            "placeholder": "​",
            "style": "IPY_MODEL_fb39a17f614e4a1f82e7d39dd27c1b75",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b4dd0adac58d4b058684e1751d89af0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5eeb94bb76141f6ba9e0665f43f1e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd464e72edf84a2bb14dca0081c764db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601929dddf2b4d598beb44662948bb53",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52af64f6897a498897add16deb32e231",
            "value": 4
          }
        },
        "c5faf72282f7493eb130652ed41386e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51d0db8b89ac4a8e8f3a34c915226997",
            "placeholder": "​",
            "style": "IPY_MODEL_5bb8bfe682b34358914c2fd44c14f555",
            "value": " 4/4 [00:14&lt;00:00,  3.03s/it]"
          }
        },
        "d7cc77a3936d4c8bb58e50130ef4e85e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df2edc009b664660a08e0f5d8a50ca19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa1861ff01524bb1a402cee40e01a7af",
              "IPY_MODEL_87c815ba99b74f5dbb673ceae2a599be",
              "IPY_MODEL_54a5113c4d6b400f8a0f3f2f47ba95c3"
            ],
            "layout": "IPY_MODEL_5242f778c16a4507928dda308c6373f7"
          }
        },
        "e6b9dda1de284a97ab1e95005cf93921": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e84b08d16ad141bea14243ba4154b5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d8b459e8e0e4624b006151709b4993c",
              "IPY_MODEL_3d11ad7d361e43b285ab4637018e5d00",
              "IPY_MODEL_145a67d712784079b572ed3108fdce4d"
            ],
            "layout": "IPY_MODEL_8cdd83193ca243c296cede606c9c3e87"
          }
        },
        "fa1861ff01524bb1a402cee40e01a7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ff5d29a433345deac2847a3b8617a56",
            "placeholder": "​",
            "style": "IPY_MODEL_b5eeb94bb76141f6ba9e0665f43f1e8d",
            "value": "Map: 100%"
          }
        },
        "fb39a17f614e4a1f82e7d39dd27c1b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
